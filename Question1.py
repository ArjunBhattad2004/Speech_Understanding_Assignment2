# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y6N79Sw36uWV-lBG3vCYNrBVxh9j-aTL

Question 1: Speech Enhancement

I. Download the VoxCeleb1 and VoxCeleb2 datasets using this link.
"""

!pip install gdown --quiet

# Define the shared drive folder link
drive_link = "https://drive.google.com/drive/folders/1qypIUgCoPfp5mCqPCbBobnw9hJKlW1Xm"
dataset_folder = "/content/VoxCeleb"

# Use gdown to download the dataset
import os
import gdown

# Function to download a Google Drive folder
def download_drive_folder(folder_link, dest_path):
    if not os.path.exists(dest_path):
        os.makedirs(dest_path)

    # Extract folder ID from link
    folder_id = folder_link.split("/")[-1]

    # Use gdown to list files and download them
    !gdown --folder {folder_link} -O {dest_path}

# Download the dataset
download_drive_folder(drive_link, dataset_folder)

# Unzip the dataset (modify if dataset is not in .zip format)
!unzip -q "/content/VoxCeleb/*.zip" -d "/content/VoxCeleb/"

# List the dataset contents
!ls -lh /content/VoxCeleb

import glob

# Define dataset path
dataset_path = "/content/VoxCeleb"

# Get list of all zip files inside vox1 and vox2
zip_files = glob.glob(os.path.join(dataset_path, "vox*/*.zip"))

# Unzip each file
for zip_file in zip_files:
    print(f"Unzipping: {zip_file} ...")
    os.system(f'unzip -q "{zip_file}" -d "{dataset_path}"')

# Verify extracted contents
!ls -lh /content/VoxCeleb

def print_directory_structure(startpath, max_depth=3, indent=2):
    for root, dirs, files in os.walk(startpath):
        level = root.replace(startpath, "").count(os.sep)
        if level >= max_depth:
            continue  # Skip deeper levels
        indent_space = " " * (indent * level)
        print(f"{indent_space}{os.path.basename(root)}/")
        subindent = " " * (indent * (level + 1))
        for f in files[:5]:  # Show only first 5 files per folder
            print(f"{subindent}{f}")

print_directory_structure("/content/VoxCeleb")

import requests

# Direct OneDrive download link (replace with your actual link)
onedrive_link = "https://azf19a.dm.files.1drv.com/y4mzDYIxx4wbZ3xnKq9tP7X7LhzwMlEi6Skoaq25kzxf0eYwjZ4Fw8nC-qwL4w7MO7uLxsSvIQjuDvUmOgTySTWpkymZ-eBuyaq-lR3akktF50gph-0mm0EnO6vNAXm6I4rRCBMQRUs0YqqbmFFWf0dKyZkv9aoWIRgggBY19cLHH8iexL4RMqisE04lZpC_J1op2POGQ5dsszOc5AAkdD_bg"
model_path = "/content/wavelm_base_plus_model.th"

# Download the model
response = requests.get(onedrive_link, allow_redirects=True)
with open(model_path, "wb") as f:
    f.write(response.content)

print("Model downloaded successfully:", model_path)

# Step 1: Install CPU version of PyTorch
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Step 2: Install additional dependencies
!pip install librosa transformers packaging

# Step 3: Install s3prl from GitHub
!pip install --no-cache-dir git+https://github.com/s3prl/s3prl.git

# Step 4: Upgrade numba and numpy (fixes compatibility issues)
!pip install -U numba numpy

pip uninstall -y torch torchvision torchaudio

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

!pip install --no-cache-dir --force-reinstall s3prl@git+https://github.com/s3prl/s3prl.git@7ab62aaf2606d83da6c71ee74e7d16e0979edbc3#egg=s3prl

import torch

device = torch.device("cpu")  # Force CPU usage
print("Using device:", device)

from s3prl.hub import wavlm_base_plus
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the pretrained WavLM Base Plus model using your checkpoint.
model = wavlm_base_plus(ckpt="/content/wavelm_base_plus_model.th").to(device)
model.eval()
print("Pretrained WavLM Base Plus model loaded successfully!")

import torchaudio
import torch.nn.functional as F

def load_audio(file_path):
    # Load audio; shape: [channels, time]
    waveform, sr = torchaudio.load(file_path)
    if sr != 16000:
        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
    # If multi-channel, average to mono.
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)
    return waveform.to(device)

def extract_embedding(model, file_path):
    waveform = load_audio(file_path)  # Expected shape: [1, time]
    # Ensure waveform is [1, time]: remove channel dim and add batch dimension.
    waveform = waveform.squeeze(0).unsqueeze(0)  # shape becomes [1, time]

    with torch.no_grad():
        out = model(waveform)
        # If the model returns a dict, check for common keys.
        if isinstance(out, dict):
            if "embedding" in out:
                tensor = out["embedding"]
            elif "last_hidden_state" in out:
                tensor = out["last_hidden_state"]
            else:
                raise ValueError("Model output missing a recognized key (embedding or last_hidden_state).")
        else:
            tensor = out
        # Assume tensor shape is [1, T, F]. To get a fixed-dimension embedding, average over T.
        if tensor.dim() == 3:
            emb = tensor.mean(dim=1)  # now shape [1, F]
        else:
            emb = tensor
        emb = emb.squeeze(0)  # shape [F]
    return emb.cpu()

from pathlib import Path

trial_file = "/content/drive/MyDrive/veri_test2.txt"
vox1_dir = Path("/content/VoxCeleb/wav")

with open(trial_file, "r") as f:
    trial_lines = [line.strip() for line in f.readlines()]

# Use a cache so each file is processed only once.
embedding_cache = {}

def get_embedding(rel_path):
    if rel_path not in embedding_cache:
        full_path = vox1_dir / rel_path
        embedding_cache[rel_path] = extract_embedding(model, str(full_path))
    return embedding_cache[rel_path]

scores = []
labels = []
for line in trial_lines:
    parts = line.split()
    if len(parts) != 3:
        continue
    label_str, file1, file2 = parts
    labels.append(int(label_str))
    emb1 = get_embedding(file1)
    emb2 = get_embedding(file2)
    # Compute cosine similarity between the two embeddings.
    sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()
    scores.append(sim)

print("Speaker verification scores computed!")

import numpy as np
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(labels, scores)
fnr = 1 - tpr
eer_index = np.nanargmin(np.abs(fpr - fnr))
eer_threshold = thresholds[eer_index]
eer = fpr[eer_index] * 100  # in percentage

# TAR@1%FAR: TPR at the largest threshold where FPR <= 0.01
idx = np.where(fpr <= 0.01)[0]
tar_at_1_far = tpr[idx[-1]] * 100 if len(idx) > 0 else 0.0

# Speaker Identification Accuracy (simple threshold classification)
pred_labels = [1 if s >= eer_threshold else 0 for s in scores]
accuracy = np.mean(np.array(pred_labels) == np.array(labels)) * 100

print(f"Pretrained Model Performance:")
print(f"EER: {eer:.2f}%")
print(f"TAR@1%FAR: {tar_at_1_far:.2f}%")
print(f"Identification Accuracy: {accuracy:.2f}%")

import os
from torch.utils.data import Dataset, DataLoader
import torchaudio

class VoxCeleb2Dataset(Dataset):
    def __init__(self, base_dir, speaker_list):
        self.samples = []  # Each sample is a tuple: (audio_path, label)
        self.speaker2label = {spk: idx for idx, spk in enumerate(speaker_list)}
        for spk in speaker_list:
            spk_dir = os.path.join(base_dir, spk)
            for root, dirs, files in os.walk(spk_dir):
                for file in files:
                    if file.lower().endswith((".wav", ".aac", ".m4a")):
                        self.samples.append((os.path.join(root, file), self.speaker2label[spk]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        waveform, sr = torchaudio.load(path)
        if sr != 16000:
            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
        if waveform.shape[0] > 1:  # Average channels if needed.
            waveform = waveform.mean(dim=0, keepdim=True)
        waveform = waveform.squeeze(0)  # shape: [time]
        return waveform, label

vox2_base = "/content/VoxCeleb/aac"
all_vox2_speakers = sorted(os.listdir(vox2_base))
train_speakers = all_vox2_speakers[:100]
test_speakers = all_vox2_speakers[100:118]

train_dataset = VoxCeleb2Dataset(vox2_base, train_speakers)
test_dataset = VoxCeleb2Dataset(vox2_base, test_speakers)
print(f"VoxCeleb2: {len(train_dataset)} training samples, {len(test_dataset)} testing samples")

!pip install peft

for name, module in model.named_modules():
    print(name)

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,  # Rank of LoRA matrices
    lora_alpha=32,  # Scaling factor
    lora_dropout=0.1,  # Dropout rate
    target_modules=[
        "self_attn.k_proj",
        "self_attn.q_proj",
        "self_attn.v_proj",
        "self_attn.out_proj",
        "fc1",
        "fc2"
    ],
    bias="none"  # Don't train biases
)

# Wrap the pretrained model with LoRA for fine-tuning.
model_ft = get_peft_model(model, lora_config)
print("LoRA applied to the model for fine-tuning!")

import torch.nn as nn
import torch.nn.functional as F

class ArcFaceLoss(nn.Module):
    def __init__(self, in_features, out_features, s=30.0, m=0.50):
        super(ArcFaceLoss, self).__init__()
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        self.s = s
        self.m = m
        nn.init.xavier_uniform_(self.weight)

    def forward(self, input, labels):
        cosine = F.linear(F.normalize(input), F.normalize(self.weight))
        theta = torch.acos(torch.clamp(cosine, -1.0, 1.0))
        target_logits = torch.cos(theta + self.m)
        one_hot = torch.zeros_like(cosine)
        one_hot.scatter_(1, labels.view(-1, 1), 1)
        output = one_hot * target_logits + (1 - one_hot) * cosine
        output *= self.s
        return F.cross_entropy(output, labels)

# Use a dummy input to determine the feature dimension.
dummy_wave = torch.randn(1, 16000).to(device)
dummy_out = model_ft(dummy_wave)

# Extract the last hidden state tensor
if isinstance(dummy_out, dict):
    dummy_out = dummy_out["last_hidden_state"]  # Extract the main tensor output

# Compute feature embedding
dummy_emb = dummy_out.mean(dim=1)  # Mean over time dimension
feature_dim = dummy_emb.shape[-1]  # Get feature size

print("Feature dimension:", feature_dim)

arcface_loss_fn = ArcFaceLoss(in_features=feature_dim, out_features=len(train_speakers)).to(device)
print("ArcFace loss defined!")

from torch.utils.data import DataLoader

def collate_fn(batch):
    # Each batch item: (waveform, label)
    waveforms, labels = zip(*batch)
    lengths = [w.shape[0] for w in waveforms]
    max_len = max(lengths)
    padded = torch.stack([F.pad(w, (0, max_len - w.shape[0])) for w in waveforms])
    return padded, torch.tensor(labels)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)
optimizer = torch.optim.AdamW(model_ft.parameters(), lr=1e-4)

num_epochs = 5
for epoch in range(num_epochs):
    model_ft.train()
    total_loss = 0.0
    for batch_waveforms, batch_labels in train_loader:
        batch_waveforms = batch_waveforms.to(device)  # shape: [batch, time]
        batch_labels = batch_labels.to(device)
        optimizer.zero_grad()
        # Model expects [batch, time]
        outputs = model_ft(batch_waveforms)  # expected shape: [batch, feature_dim, T]
        embeddings = outputs.mean(dim=-1)     # average over time → [batch, feature_dim]
        loss = arcface_loss_fn(embeddings, batch_labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}")
print("Fine-tuning completed!")

# Clear embedding cache
embedding_cache = {}

def get_embedding_ft(rel_path):
    if rel_path not in embedding_cache:
        full_path = vox1_dir / rel_path
        embedding_cache[rel_path] = extract_embedding(model_ft, str(full_path))
    return embedding_cache[rel_path]

scores_ft = []
for line in trial_lines:
    parts = line.split()
    if len(parts) != 3:
        continue
    label_str, file1, file2 = parts
    emb1 = get_embedding_ft(file1)
    emb2 = get_embedding_ft(file2)
    sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()
    scores_ft.append(sim)

# Compute metrics for fine-tuned model
from sklearn.metrics import roc_curve
fpr_ft, tpr_ft, thresholds_ft = roc_curve(labels, scores_ft)
fnr_ft = 1 - tpr_ft
eer_index_ft = np.nanargmin(np.abs(fpr_ft - fnr_ft))
eer_threshold_ft = thresholds_ft[eer_index_ft]
eer_ft = fpr_ft[eer_index_ft] * 100
idx_ft = np.where(fpr_ft <= 0.01)[0]
tar_at_1_far_ft = tpr_ft[idx_ft[-1]] * 100 if len(idx_ft) > 0 else 0.0
pred_labels_ft = [1 if s >= eer_threshold_ft else 0 for s in scores_ft]
accuracy_ft = np.mean(np.array(pred_labels_ft) == np.array(labels)) * 100

print(f"Fine-Tuned Model Performance:")
print(f"EER: {eer_ft:.2f}%")
print(f"TAR@1%FAR: {tar_at_1_far_ft:.2f}%")
print(f"Identification Accuracy: {accuracy_ft:.2f}%")

"""Part3"""

!pip install soundfile tqdm pesq museval

import os
import random
import torchaudio
import soundfile as sf
from pathlib import Path
from tqdm import tqdm
import numpy as np

def mix_utterances(file1, file2, snr_db=0):
    """Mix two utterances with a specified SNR (signal-to-noise ratio) in dB."""
    wav1, sr1 = torchaudio.load(file1)
    wav2, sr2 = torchaudio.load(file2)

    if sr1 != sr2:
        raise ValueError("Sample rates don't match!")

    # Make mono
    if wav1.shape[0] > 1:
        wav1 = wav1.mean(dim=0, keepdim=True)
    if wav2.shape[0] > 1:
        wav2 = wav2.mean(dim=0, keepdim=True)

    # Truncate to shorter one
    min_len = min(wav1.shape[1], wav2.shape[1])
    wav1 = wav1[:, :min_len]
    wav2 = wav2[:, :min_len]

    # Adjust SNR
    s1 = wav1.numpy().flatten()
    s2 = wav2.numpy().flatten()
    rms1 = np.sqrt(np.mean(s1**2))
    rms2 = np.sqrt(np.mean(s2**2))
    scalar = rms1 / (10**(snr_db / 20)) / rms2
    s2_scaled = s2 * scalar

    mix = s1 + s2_scaled
    mix = mix / max(abs(mix))  # normalize

    return mix, s1, s2

vox2_base = "/content/VoxCeleb/aac"
all_speakers = sorted(os.listdir(vox2_base))
train_speakers = all_speakers[:50]
test_speakers = all_speakers[50:100]

def collect_audio_files(speaker_list):
    all_files = []
    for spk in speaker_list:
        spk_dir = Path(vox2_base) / spk
        for root, dirs, files in os.walk(spk_dir):
            for file in files:
                if file.endswith(".m4a"):
                    all_files.append(os.path.join(root, file))
    return all_files

train_files = collect_audio_files(train_speakers)
test_files = collect_audio_files(test_speakers)

vox2_base = "/content/VoxCeleb/aac"
all_speakers = sorted(os.listdir(vox2_base))
train_speakers = all_speakers[:50]
test_speakers = all_speakers[50:100]

def collect_audio_files(speaker_list):
    all_files = []
    for spk in speaker_list:
        spk_dir = Path(vox2_base) / spk
        for root, dirs, files in os.walk(spk_dir):
            for file in files:
                if file.endswith(".m4a"):
                    all_files.append(os.path.join(root, file))
    return all_files

train_files = collect_audio_files(train_speakers)
test_files = collect_audio_files(test_speakers)

def create_mixed_dataset(file_list, save_dir, num_pairs=500, split='train'):
    os.makedirs(save_dir, exist_ok=True)
    for i in tqdm(range(num_pairs), desc=f"Mixing {split} data"):
        f1, f2 = random.sample(file_list, 2)
        mix, src1, src2 = mix_utterances(f1, f2)
        sf.write(f"{save_dir}/mix_{i}.wav", mix, 16000)
        sf.write(f"{save_dir}/src1_{i}.wav", src1, 16000)
        sf.write(f"{save_dir}/src2_{i}.wav", src2, 16000)

create_mixed_dataset(train_files, "/content/multi_speaker/train", num_pairs=500, split="train")
create_mixed_dataset(test_files, "/content/multi_speaker/test", num_pairs=100, split="test")

"""Part4"""

!pip install git+https://github.com/asteroid-team/asteroid

from asteroid.models import Sepformer
sep_model = Sepformer.from_pretrained("JorisCos/SepFormer-wsj02mix").to("cpu")
sep_model.eval()
print("SepFormer loaded!")

def separate_audio(input_path, output_dir):
    mixture, sr = torchaudio.load(input_path)
    if mixture.shape[0] > 1:
        mixture = mixture.mean(dim=0, keepdim=True)
    est_sources = sep_model.separate(mixture)
    for i, source in enumerate(est_sources):
        out_path = os.path.join(output_dir, f"{Path(input_path).stem}_est{i+1}.wav")
        torchaudio.save(out_path, source.unsqueeze(0), sr)

# Run on test set
output_dir = "/content/multi_speaker/test_sep"
os.makedirs(output_dir, exist_ok=True)
test_mix_dir = "/content/multi_speaker/test"

for file in tqdm(os.listdir(test_mix_dir)):
    if file.startswith("mix_") and file.endswith(".wav"):
        separate_audio(os.path.join(test_mix_dir, file), output_dir)

from pesq import pesq
from museval.metrics import bss_eval_sources

def evaluate_sources(ref1, ref2, est1, est2):
    sdr, sir, sar, _ = bss_eval_sources(np.vstack([ref1, ref2]), np.vstack([est1, est2]))
    pesq_score1 = pesq(16000, ref1, est1, 'wb')
    pesq_score2 = pesq(16000, ref2, est2, 'wb')
    return sdr, sir, sar, (pesq_score1 + pesq_score2) / 2

def identify_speaker(model_used, sep_audio_path, speaker_list):
    emb = extract_embedding(model_used, sep_audio_path)
    sims = []
    for spk in speaker_list:
        ref_files = collect_audio_files([spk])
        ref_embs = [extract_embedding(model_used, f) for f in random.sample(ref_files, 3)]
        ref_mean = torch.stack(ref_embs).mean(dim=0)
        sim = F.cosine_similarity(emb.unsqueeze(0), ref_mean.unsqueeze(0)).item()
        sims.append(sim)
    return speaker_list[np.argmax(sims)]

def speaker_enhancement_pipeline(mix_audio_path):
    separated = sep_model.separate(torchaudio.load(mix_audio_path)[0])
    enhanced_speakers = []
    for source in separated:
        # Save to temp file
        temp_path = "/tmp/temp.wav"
        torchaudio.save(temp_path, source.unsqueeze(0), 16000)
        predicted_spk = identify_speaker(model_ft, temp_path, test_speakers)
        enhanced_speakers.append((predicted_spk, source))
    return enhanced_speakers

from torch.utils.data import DataLoader

def collate_fn(batch):
    waveforms, labels = zip(*batch)
    lengths = [w.shape[0] for w in waveforms]
    max_len = max(lengths)
    padded = torch.stack([F.pad(w, (0, max_len - w.shape[0])) for w in waveforms])
    return padded, torch.tensor(labels)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)
optimizer = torch.optim.AdamW(model_ft.parameters(), lr=1e-4)

num_epochs = 5
for epoch in range(num_epochs):
    model_ft.train()
    total_loss = 0.0
    for batch_waveforms, batch_labels in train_loader:
        batch_waveforms = batch_waveforms.to(device)
        batch_labels = batch_labels.to(device)
        optimizer.zero_grad()
        outputs = model_ft(batch_waveforms)
        embeddings = outputs.mean(dim=-1)
        loss = arcface_loss_fn(embeddings, batch_labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}")

print("Fine-tuning completed!")

from sklearn.metrics import roc_curve
import numpy as np

embedding_cache = {}

def get_embedding_ft(rel_path):
    if rel_path not in embedding_cache:
        full_path = vox1_dir / rel_path
        embedding_cache[rel_path] = extract_embedding(model_ft, str(full_path))
    return embedding_cache[rel_path]

scores_ft = []
for line in trial_lines:
    parts = line.split()
    if len(parts) != 3:
        continue
    label_str, file1, file2 = parts
    emb1 = get_embedding_ft(file1)
    emb2 = get_embedding_ft(file2)
    sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()
    scores_ft.append(sim)

fpr_ft, tpr_ft, thresholds_ft = roc_curve(labels, scores_ft)
fnr_ft = 1 - tpr_ft
eer_index_ft = np.nanargmin(np.abs(fpr_ft - fnr_ft))
eer_threshold_ft = thresholds_ft[eer_index_ft]
eer_ft = fpr_ft[eer_index_ft] * 100
idx_ft = np.where(fpr_ft <= 0.01)[0]
tar_at_1_far_ft = tpr_ft[idx_ft[-1]] * 100 if len(idx_ft) > 0 else 0.0
pred_labels_ft = [1 if s >= eer_threshold_ft else 0 for s in scores_ft]
accuracy_ft = np.mean(np.array(pred_labels_ft) == np.array(labels)) * 100

print(f"Fine-Tuned Model Performance:")
print(f"EER: {eer_ft:.2f}%")
print(f"TAR@1%FAR: {tar_at_1_far_ft:.2f}%")
print(f"Identification Accuracy: {accuracy_ft:.2f}%")